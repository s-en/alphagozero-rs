{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a9bf1f2e6bcf5a5ddc2db4e74af98a8bb6c9f182239da13cf8d6aee21f00f13f",
   "display_name": "Python 3.8.5 64-bit ('py38': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "a9bf1f2e6bcf5a5ddc2db4e74af98a8bb6c9f182239da13cf8d6aee21f00f13f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RecursiveScriptModule(\n  original_name=DualNet\n  (conv1): RecursiveScriptModule(original_name=Conv2d)\n  (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n  (relu): RecursiveScriptModule(original_name=ReLU)\n  (layers): RecursiveScriptModule(\n    original_name=Sequential\n    (0): RecursiveScriptModule(\n      original_name=BasicBlock\n      (conv1): RecursiveScriptModule(original_name=Conv2d)\n      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n      (relu): RecursiveScriptModule(original_name=ReLU)\n      (conv2): RecursiveScriptModule(original_name=Conv2d)\n      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n      (downsample): RecursiveScriptModule(original_name=Sequential)\n    )\n    (1): RecursiveScriptModule(\n      original_name=BasicBlock\n      (conv1): RecursiveScriptModule(original_name=Conv2d)\n      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n      (relu): RecursiveScriptModule(original_name=ReLU)\n      (conv2): RecursiveScriptModule(original_name=Conv2d)\n      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n      (downsample): RecursiveScriptModule(original_name=Sequential)\n    )\n    (2): RecursiveScriptModule(\n      original_name=BasicBlock\n      (conv1): RecursiveScriptModule(original_name=Conv2d)\n      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n      (relu): RecursiveScriptModule(original_name=ReLU)\n      (conv2): RecursiveScriptModule(original_name=Conv2d)\n      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n      (downsample): RecursiveScriptModule(original_name=Sequential)\n    )\n    (3): RecursiveScriptModule(\n      original_name=BasicBlock\n      (conv1): RecursiveScriptModule(original_name=Conv2d)\n      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n      (relu): RecursiveScriptModule(original_name=ReLU)\n      (conv2): RecursiveScriptModule(original_name=Conv2d)\n      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n      (downsample): RecursiveScriptModule(original_name=Sequential)\n    )\n    (4): RecursiveScriptModule(\n      original_name=BasicBlock\n      (conv1): RecursiveScriptModule(original_name=Conv2d)\n      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n      (relu): RecursiveScriptModule(original_name=ReLU)\n      (conv2): RecursiveScriptModule(original_name=Conv2d)\n      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n      (downsample): RecursiveScriptModule(original_name=Sequential)\n    )\n  )\n  (pi): RecursiveScriptModule(\n    original_name=Sequential\n    (0): RecursiveScriptModule(original_name=Conv2d)\n    (1): RecursiveScriptModule(original_name=BatchNorm2d)\n    (2): RecursiveScriptModule(original_name=ReLU)\n    (3): RecursiveScriptModule(original_name=Flatten)\n    (4): RecursiveScriptModule(original_name=Linear)\n    (5): RecursiveScriptModule(original_name=LogSoftmax)\n  )\n  (v): RecursiveScriptModule(\n    original_name=Sequential\n    (0): RecursiveScriptModule(original_name=Conv2d)\n    (1): RecursiveScriptModule(original_name=BatchNorm2d)\n    (2): RecursiveScriptModule(original_name=ReLU)\n    (3): RecursiveScriptModule(original_name=Flatten)\n    (4): RecursiveScriptModule(original_name=Linear)\n    (5): RecursiveScriptModule(original_name=ReLU)\n    (6): RecursiveScriptModule(original_name=Linear)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "loaded = torch.jit.load('temp/best.pt')\n",
    "print(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph(%x.1 : Float(16, 9, 5, 5, strides=[225, 25, 5, 1], requires_grad=0, device=cpu),\n      %pi.4.bias : Float(26, strides=[1], requires_grad=0, device=cuda:0),\n      %pi.4.weight : Float(26, 800, strides=[800, 1], requires_grad=0, device=cuda:0),\n      %v.4.bias : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %v.4.weight : Float(32, 75, strides=[75, 1], requires_grad=0, device=cuda:0),\n      %v.6.bias : Float(1, strides=[1], requires_grad=0, device=cuda:0),\n      %v.6.weight : Float(1, 32, strides=[32, 1], requires_grad=0, device=cuda:0),\n      %129 : Float(32, 9, 3, 3, strides=[81, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %130 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %132 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %133 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %135 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %136 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %138 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %139 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %141 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %142 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %144 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %145 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %147 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %148 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %150 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %151 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %153 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %154 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %156 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %157 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %159 : Float(32, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %160 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %162 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %163 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %165 : Float(3, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %166 : Float(3, strides=[1], requires_grad=0, device=cuda:0)):\n  %72 : Long(4, strides=[1], device=cpu) = onnx::Constant[value=-1  9  5  5 [ CPULongType{4} ]]()\n  %73 : Float(16, 9, 5, 5, strides=[225, 25, 5, 1], device=cpu) = onnx::Reshape(%x.1, %72) # dualnet.py:77:0\n  %128 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%73, %129, %130)\n  %76 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%128) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %131 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%76, %132, %133)\n  %79 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%131) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %134 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%79, %135, %136)\n  %82 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Add(%134, %76) # dualnet.py:33:0\n  %83 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%82) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %137 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%83, %138, %139)\n  %86 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%137) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %140 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%86, %141, %142)\n  %89 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Add(%140, %83) # dualnet.py:33:0\n  %90 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%89) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %143 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%90, %144, %145)\n  %93 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%143) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %146 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%93, %147, %148)\n  %96 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Add(%146, %90) # dualnet.py:33:0\n  %97 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%96) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %149 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%97, %150, %151)\n  %100 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%149) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %152 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%100, %153, %154)\n  %103 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Add(%152, %97) # dualnet.py:33:0\n  %104 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%103) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %155 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%104, %156, %157)\n  %107 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%155) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %158 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%107, %159, %160)\n  %110 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Add(%158, %104) # dualnet.py:33:0\n  %111 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%110) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %161 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%111, %162, %163)\n  %114 : Float(16, 32, 5, 5, strides=[800, 25, 5, 1], device=cpu) = onnx::Relu(%161) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %115 : Float(16, 800, strides=[800, 1], device=cpu) = onnx::Flatten[axis=1](%114) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:40:0\n  %116 : Float(16, 26, strides=[26, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%115, %pi.4.weight, %pi.4.bias) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n  %117 : Float(16, 26, strides=[26, 1], device=cpu) = onnx::LogSoftmax[axis=1](%116) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1672:0\n  %118 : Float(16, 26, strides=[26, 1], device=cpu) = onnx::Exp(%117) # dualnet.py:82:0\n  %164 : Float(16, 3, 5, 5, strides=[75, 25, 5, 1], device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%111, %165, %166)\n  %121 : Float(16, 3, 5, 5, strides=[75, 25, 5, 1], device=cpu) = onnx::Relu(%164) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %122 : Float(16, 75, strides=[75, 1], device=cpu) = onnx::Flatten[axis=1](%121) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:40:0\n  %123 : Float(16, 32, strides=[32, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%122, %v.4.weight, %v.4.bias) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n  %124 : Float(16, 32, strides=[32, 1], device=cpu) = onnx::Relu(%123) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1204:0\n  %125 : Float(16, 1, strides=[1, 1], device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%124, %v.6.weight, %v.6.bias) # C:\\Users\\sen\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n  %126 : Float(16, 1, strides=[1, 1], device=cpu) = onnx::Tanh(%125) # dualnet.py:83:0\n  %127 : Float(16, 27, strides=[27, 1], requires_grad=0, device=cpu) = onnx::Concat[axis=1](%118, %126) # dualnet.py:84:0\n  return (%127)\n\n"
     ]
    }
   ],
   "source": [
    "loaded.eval()\n",
    "dummy_input = torch.randn((16, 9, 5, 5))\n",
    "torch.onnx.export(loaded, dummy_input, \"dualnet.onnx\", example_outputs=torch.rand((16, 27)), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model.pb\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.autograph.experimental.do_not_convert\n",
    "onnx_model = onnx.load('dualnet.onnx')\n",
    "\n",
    "tf_rep = prepare(onnx_model, device='cpu')\n",
    "tf_rep.export_graph('model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<ConcreteFunction signature_wrapper(x.1) at 0x25F0F0E9D30>\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.saved_model.load('model.pb')\n",
    "f = new_model.signatures[\"serving_default\"]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'tensorflowjs_converter' �́A�����R�}���h�܂��͊O���R�}���h�A\n����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names='output_0' model.pb js_model --quantize_float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}